{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# packages\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import sys\n",
        "import scipy.stats as stats\n",
        "import networkx as nx\n",
        "import tensorflow as tf\n",
        "from typing import List, Dict, Any, Union\n",
        "from itertools import product\n",
        "from typing import List, Dict, Any, Union\n",
        "from itertools import product\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 384
        },
        "id": "eYn6Mt_aSE4R",
        "outputId": "a9424204-ce57-4f9e-b034-a560a8e9f041"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'keras_tuner'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-b128e62f24fd>\u001b[0m in \u001b[0;36m<cell line: 19>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDense\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mStandardScaler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mkeras_tuner\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mkt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'keras_tuner'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Plan:\n",
        "\n",
        "# Split the data into data[train, test, validation]\n",
        "# remove WLB score from data\n",
        "# Create an MLP using the train and test data\n",
        "# use the MLP to get embeddings of the validation data\n",
        "# use that to make the similarity graph (size is same as validation set)\n",
        "# Clustering on similarity graph (louvain and spectral methods) (GNN)\n",
        "# Analyze the results of the clusters: Get the WLB scores of each cluster, see similarities in each cluster\n",
        "\n",
        "# research:\n",
        "# graph clustering based prediction tasks\n",
        "# node classification\n",
        "# node property prediction\n",
        "# interpretable prediction for graphs\n",
        "# FAISS -- pairwise similarity computation\n",
        "\n",
        "\n",
        "# expectation:\n",
        "# subset of factors to analyze\n"
      ],
      "metadata": {
        "id": "O3_WvfUj5bXX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: This MIGHT be different for yall, so just pay attention to this.\n",
        "GOOGLE_DRIVE_PATH_AFTER_MYDRIVE = \"576_Project/data\"\n",
        "GOOGLE_DRIVE_PATH = os.path.join(\"drive\", \"My Drive\", GOOGLE_DRIVE_PATH_AFTER_MYDRIVE)\n",
        "sys.path.append(GOOGLE_DRIVE_PATH)\n",
        "\n",
        "# Should just see Wellbeing_and_lifestyle_data_Kaggle.csv here\n",
        "print(os.listdir(GOOGLE_DRIVE_PATH))\n",
        "\n",
        "assert 'Wellbeing_and_lifestyle_data_Kaggle.csv' in list(os.listdir(GOOGLE_DRIVE_PATH)), \"Data not found, check your file paths!\"\n",
        "\n",
        "full_data_path = os.path.join(\"drive\", \"My Drive\", GOOGLE_DRIVE_PATH_AFTER_MYDRIVE, \"Wellbeing_and_lifestyle_data_Kaggle.csv\")\n",
        "df = pd.read_csv(full_data_path)\n",
        "print(df.head())"
      ],
      "metadata": {
        "id": "k9LAFJ5BQzxR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WQwemdZoHrhs"
      },
      "outputs": [],
      "source": [
        "def load_and_preprocess_data(google_drive_path: str, filename: str):\n",
        "    '''\n",
        "    Inputs:\n",
        "    - google_drive_path: str, path to the dataset directory.\n",
        "    - filename: str, name of the dataset file.\n",
        "\n",
        "    Outputs:\n",
        "    - DataFrame: cleaned and preprocessed dataset.\n",
        "    '''\n",
        "    try:\n",
        "        # Combine paths to form the full data path\n",
        "        full_data_path = os.path.join(google_drive_path, filename)\n",
        "\n",
        "        # Check if the file exists in the specified directory\n",
        "        if filename not in os.listdir(google_drive_path):\n",
        "            raise FileNotFoundError(f\"Data file '{filename}' not found in the specified directory.\")\n",
        "\n",
        "        # Load the dataset\n",
        "        df = pd.read_csv(full_data_path)\n",
        "        print(\"Dataset loaded successfully.\")\n",
        "\n",
        "        # Preprocessing: Type conversion\n",
        "        df[\"AGE\"] = df['AGE'].map({\"Less than 20\": 0, \"21 to 35\": 1, \"36 to 50\": 2, \"51 or more\": 3}).fillna(0)\n",
        "        df[\"GENDER\"] = df[\"GENDER\"].map({\"Female\": 0, \"Male\": 1}).fillna(0)\n",
        "\n",
        "        # Convert columns to numeric where possible, coercing errors to NaN\n",
        "        df = df.apply(pd.to_numeric, errors='coerce')\n",
        "\n",
        "        print(\"Preprocessing completed.\")\n",
        "        return df\n",
        "\n",
        "    except FileNotFoundError as e:\n",
        "        print(e)\n",
        "    except pd.errors.EmptyDataError:\n",
        "        print(\"Error: File is empty.\")\n",
        "    except Exception as e:\n",
        "        print(f\"An unexpected error occurred: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "GOOGLE_DRIVE_PATH_AFTER_MYDRIVE = \"drive/My Drive/576_Project/data\"\n",
        "FILENAME = \"Wellbeing_and_lifestyle_data_Kaggle.csv\"\n",
        "\n",
        "df = load_and_preprocess_data(GOOGLE_DRIVE_PATH_AFTER_MYDRIVE, FILENAME)\n",
        "# print(df.head())\n",
        "\n",
        "\n",
        "# correlation matrix\n",
        "plt.figure(figsize=(15,8))\n",
        "data = df.drop('Timestamp', axis=1)\n",
        "# print(data)\n",
        "sns.heatmap(data=data.corr(), annot=True, fmt='0.3f', cmap='GnBu');"
      ],
      "metadata": {
        "id": "OFGzbXcWR-sL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def split_data(df, target_column, test_size=0.15, val_size=0.15, random_state=None):\n",
        "    \"\"\"\n",
        "    Splits the data into training, test, and validation sets.\n",
        "\n",
        "    Parameters:\n",
        "    - df (pd.DataFrame): The DataFrame containing the data.\n",
        "    - target_column (str): The name of the column representing the target variable.\n",
        "    - test_size (float): The proportion of the dataset to include in the test split (default is 0.2).\n",
        "    - val_size (float): The proportion of the dataset to include in the validation split (default is 0.1).\n",
        "    - random_state (int): Random state for reproducibility (default is None).\n",
        "\n",
        "    Returns:\n",
        "    - X_train, X_val, X_test, y_train, y_val, y_test: Split data ready for model training.\n",
        "    \"\"\"\n",
        "\n",
        "    X = df.drop(columns=[target_column])\n",
        "    y = df[target_column]\n",
        "\n",
        "    X_train, X_temp, y_train, y_temp = train_test_split(\n",
        "        X, y, test_size=(test_size + val_size), random_state=random_state\n",
        "    )\n",
        "\n",
        "    val_proportion = val_size / (test_size + val_size)\n",
        "\n",
        "    X_val, X_test, y_val, y_test = train_test_split(\n",
        "        X_temp, y_temp, test_size=val_proportion, random_state=random_state\n",
        "    )\n",
        "\n",
        "    return X_train, X_val, X_test, y_train, y_val, y_test\n"
      ],
      "metadata": {
        "id": "Mcrq__MlIImW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_val, X_test, y_train, y_val, y_test = split_data(df, target_column='WORK_LIFE_BALANCE_SCORE', test_size=0.15, val_size=0.15, random_state=1)\n"
      ],
      "metadata": {
        "id": "N1wLVTwD7g8f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_mlp_model(X_train, y_train, X_val, y_val, input_dim, epochs=50, batch_size=32):\n",
        "    \"\"\"\n",
        "    Creates and trains a Multi-Layer Perceptron (MLP) model.\n",
        "\n",
        "    Parameters:\n",
        "    - X_train (np.array or pd.DataFrame): Training data features.\n",
        "    - y_train (np.array or pd.Series): Training data labels.\n",
        "    - X_val (np.array or pd.DataFrame): Validation data features.\n",
        "    - y_val (np.array or pd.Series): Validation data labels.\n",
        "    - input_dim (int): Number of input features.\n",
        "    - epochs (int): Number of epochs for training (default is 50).\n",
        "    - batch_size (int): Size of training batches (default is 32).\n",
        "\n",
        "    Returns:\n",
        "    - model (tf.keras.Model): Trained MLP model.\n",
        "    \"\"\"\n",
        "    #standardizes data, not sure if we need to change this for some of the parameters that are binary\n",
        "    scaler = StandardScaler()\n",
        "    X_train = scaler.fit_transform(X_train)\n",
        "    X_val = scaler.transform(X_val)\n",
        "\n",
        "    model = Sequential([\n",
        "        Dense(64, activation='relu', input_dim=input_dim),\n",
        "        Dense(32, activation='relu'),\n",
        "        Dense(1)  # Adjust this based on your output requirements (e.g., activation='sigmoid' for binary classification)\n",
        "    ])\n",
        "\n",
        "    model.compile(optimizer='adam', loss='mse', metrics=['mae'])  # Adjust 'loss' based on the type of problem\n",
        "\n",
        "    model.fit(\n",
        "        X_train, y_train,\n",
        "        validation_data=(X_val, y_val),\n",
        "        epochs=epochs,\n",
        "        batch_size=batch_size,\n",
        "        verbose=2\n",
        "    )\n",
        "\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "R0yu7v2eINXJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = create_mlp_model(X_train, y_train, X_val, y_val, input_dim=X_train.shape[1])"
      ],
      "metadata": {
        "id": "uCQBHFigIn-T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the partition rules here\n",
        "# Options for setting rules are:\n",
        "#   - Values: define the boundaries between bins as values\n",
        "#   - Percentile: define the boundaries between bins as percentiles\n",
        "#   - Exact: define the exact values a category can take as a list\n",
        "\n",
        "partition_rules = {\n",
        "    \"WORK_LIFE_BALANCE_SCORE\": {\"type\": \"percentile\", \"bins\": [0, 10, 90, 100]},\n",
        "    \"GENDER\": {\"type\": \"exact\", \"bins\": [0, 1]},  # Exact values for gender\n",
        "    \"BMI_RANGE\": {\"type\": \"exact\", \"bins\": [1, 2]} # Exact values for BMI because there are only 2, not sure why\n",
        "}\n",
        "\n",
        "# Assume 'df' is your data DataFrame\n",
        "partitions = partition_data(df, partition_rules)\n",
        "\n",
        "# Example: print the first few rows of some partitions\n",
        "for partition_name, partition_df in partitions.items():\n",
        "    print(f\"Partition: {partition_name}\")\n",
        "    print(f\"{partition_name}: {len(partition_df)} items\")\n",
        "    # print(partition_df.head())"
      ],
      "metadata": {
        "id": "IJ3KAJW5Z3NK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## 3. Correlation Calculation\n",
        "def calculate_correlation(data: pd.DataFrame, method: str = \"pearson\") -> pd.DataFrame:\n",
        "    '''\n",
        "    Inputs:\n",
        "    - data: DataFrame for which to compute correlations.\n",
        "    - method: str, correlation method (\"pearson\", \"spearman\", \"kendall\").\n",
        "\n",
        "    Outputs:\n",
        "    - DataFrame: correlation matrix.\n",
        "    '''\n",
        "    return data.corr(method=method)"
      ],
      "metadata": {
        "id": "Isa_-TxFJwmL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def is_significant_correlation(correlation_value, sample_size, alpha=0.05):\n",
        "    \"\"\"\n",
        "    Runs a t-test to determine if a correlation value is significantly different from 0.\n",
        "\n",
        "    Parameters:\n",
        "    - correlation_value: The correlation coefficient to test.\n",
        "    - sample_size: The number of samples in the partition.\n",
        "    - alpha: Significance level (default is 0.05).\n",
        "\n",
        "    Returns:\n",
        "    - 1 if the correlation is significant, otherwise 0.\n",
        "    \"\"\"\n",
        "    # Degrees of freedom for the t-test\n",
        "    df = sample_size - 2\n",
        "\n",
        "    # Calculate the t-statistic\n",
        "    t_statistic = correlation_value * np.sqrt(df / (1 - correlation_value**2))\n",
        "\n",
        "    # Calculate the two-tailed p-value\n",
        "    p_value = 2 * (1 - stats.t.cdf(abs(t_statistic), df))\n",
        "\n",
        "    # Return 1 if p-value is less than alpha, indicating significance\n",
        "    return 1 if p_value < alpha else 0\n",
        "\n",
        "def generate_significance_graph(correlation_matrix, sample_size):\n",
        "    \"\"\"\n",
        "    Generates a matrix of 1s and 0s indicating whether each correlation\n",
        "    in the correlation matrix is significant.\n",
        "\n",
        "    Parameters:\n",
        "    - correlation_matrix: The partition dataframe to analyze.\n",
        "\n",
        "    Returns:\n",
        "    - A DataFrame of 1s and 0s indicating significance.\n",
        "    \"\"\"\n",
        "    # Create a matrix to store significance results, initially copying the correlation matrix\n",
        "    significance_matrix = correlation_matrix.copy()\n",
        "\n",
        "    # Iterate over the matrix to modify only off-diagonal elements\n",
        "    for i in range(correlation_matrix.shape[0]):\n",
        "        for j in range(correlation_matrix.shape[1]):\n",
        "            if i != j:  # Skip diagonal elements\n",
        "                correlation_value = correlation_matrix.iloc[i, j]\n",
        "                if not np.isnan(correlation_value):\n",
        "                    significance_matrix.iloc[i, j] = is_significant_correlation(correlation_value, sample_size)\n",
        "                else:\n",
        "                    significance_matrix.iloc[i, j] = 0\n",
        "            else:\n",
        "                significance_matrix.iloc[i, j] = 0  # Set diagonal to 0\n",
        "\n",
        "    return significance_matrix\n"
      ],
      "metadata": {
        "id": "zOm1ENwvqCsZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def visualize_graph(adj_matrix):\n",
        "    \"\"\"\n",
        "    Visualizes a graph from an adjacency matrix.\n",
        "\n",
        "    Parameters:\n",
        "    - adj_matrix: A 2D dataframe where 1s represent edges and 0s represent no edges.\n",
        "    \"\"\"\n",
        "    # Create a graph from the adjacency matrix\n",
        "    G = nx.from_pandas_adjacency(adj_matrix)\n",
        "\n",
        "    # Draw the graph\n",
        "    plt.figure(figsize=(15, 8))\n",
        "    nx.draw(G, with_labels=True, node_color='skyblue', node_size=1000, font_size=8, font_weight='bold', edge_color='gray')\n",
        "    plt.title(\"Graph Visualization\")\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "CG__99RwqF2o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for partition_name, partition_df in partitions.items():\n",
        "    print(f\"Partition: {partition_name}\")\n",
        "    print(f\"{partition_name}: {len(partition_df)} items\")\n",
        "\n",
        "    partition_columns = list(partition_rules.keys())\n",
        "\n",
        "    # Drop the columns used for partitioning from the data\n",
        "    filtered_partition_df = partition_df.drop(columns=partition_columns, errors='ignore')\n",
        "    filtered_partition_df = filtered_partition_df.drop('Timestamp', axis=1)\n",
        "\n",
        "    sample_size = len(filtered_partition_df)\n",
        "    # print(partition_df.head())\n",
        "    plt.figure(figsize=(15,8))\n",
        "    correlation_matrix = calculate_correlation(filtered_partition_df)\n",
        "    sns.heatmap(data=correlation_matrix, annot=True, fmt='0.3f', cmap='GnBu')\n",
        "    significance_graph = generate_significance_graph(correlation_matrix, sample_size)\n",
        "    # print(significance_graph)\n",
        "    visualize_graph(significance_graph)\n",
        "    break\n"
      ],
      "metadata": {
        "id": "CDcAcmMpqH11"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## 6. Clustering Execution\n",
        "def perform_clustering(significance_graph: pd.Dataframe, method: Callable, params: Dict[str, Any]) -> Dict[int, List[str]]:\n",
        "    '''\n",
        "    Inputs:\n",
        "    - graph: Graph, correlation graph.\n",
        "    - method: Callable, clustering method function.\n",
        "    - params: Dict, parameters for the chosen clustering method.\n",
        "\n",
        "    Outputs:\n",
        "    - Dictionary mapping cluster IDs to lists of factor names.\n",
        "    '''\n",
        "    pass\n"
      ],
      "metadata": {
        "id": "tEm55uCKJ7U0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## 7. Graph Visualization\n",
        "def visualize_graph(graph: Graph, clusters: Dict[int, List[str]], output_file: str) -> None:\n",
        "    '''\n",
        "    Inputs:\n",
        "    - graph: Graph, correlation graph.\n",
        "    - clusters: Dict, resulting clusters.\n",
        "    - output_file: str, path for saving the visualization.\n",
        "\n",
        "    Outputs:\n",
        "    - None (saves the visualization to a file).\n",
        "    '''\n",
        "    pass"
      ],
      "metadata": {
        "id": "bJp0hjooKFQa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## 8. Cluster Analysis and Comparison\n",
        "def analyze_clusters(clusters: Dict[int, List[str]], partitions: Dict[str, DataFrame]) -> DataFrame:\n",
        "    '''\n",
        "    Inputs:\n",
        "    - clusters: Dict, cluster results.\n",
        "    - partitions: Dict, different subsets of the dataset.\n",
        "\n",
        "    Outputs:\n",
        "    - DataFrame summarizing key findings from clusters across partitions.\n",
        "    '''\n",
        "    pass"
      ],
      "metadata": {
        "id": "qfw30jbjKHhj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## 9. Evaluation of Correlation Methods\n",
        "def evaluate_correlation_methods(data: DataFrame, methods: List[str], partitions: Dict[str, DataFrame]) -> DataFrame:\n",
        "    '''\n",
        "    Inputs:\n",
        "    - data: DataFrame, original dataset.\n",
        "    - methods: List of correlation methods.\n",
        "    - partitions: Dict of data partitions.\n",
        "\n",
        "    Outputs:\n",
        "    - DataFrame summarizing how correlation structures vary across methods.\n",
        "    '''\n",
        "    pass"
      ],
      "metadata": {
        "id": "HMNbcpHzKKcY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## 10. Evaluation of Clustering Methods\n",
        "def louvain_clustering(graph: Graph, params: Dict[str, Any], weighted=False) -> Dict[int, List[str]]:\n",
        "  if weighted:\n",
        "    pass\n",
        "  else:\n",
        "    pass\n",
        "def kmeans_clustering(graph: Graph, params: Dict[str, Any]) -> Dict[int, List[str]]:\n",
        "  pass\n",
        "def gn_clustering(graph: Graph, params: Dict[str, Any]) -> Dict[int, List[str]]:\n",
        "  pass\n",
        "\n",
        "def evaluate_clustering_methods(graph: Graph, methods: List[str], params: Dict[str, Dict[str, Any]]) -> DataFrame:\n",
        "    '''\n",
        "    Inputs:\n",
        "    - graph: Graph, correlation graph.\n",
        "    - methods: List of clustering methods to test.\n",
        "    - params: Dict, parameters for each clustering method.\n",
        "\n",
        "    Outputs:\n",
        "    - DataFrame comparing performance and output of clustering methods.\n",
        "    '''\n",
        "    # louvain & GN & k-means\n",
        "    louvain_result = louvain_clustering(graph, params[\"louvain\"])\n",
        "    gn_result = gn_clustering(graph, params[\"GN\"])\n",
        "    k_means_result = kmeans_clustering(graph, params[\"k-means\"])\n",
        "    pass"
      ],
      "metadata": {
        "id": "d4O8Svc_KPXH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}